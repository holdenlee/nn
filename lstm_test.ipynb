{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nn-utilities\n",
    "\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from collections import OrderedDict\n",
    "\n",
    "def numpy_floatX(data):\n",
    "    return numpy.asarray(data, dtype=config.floatX)\n",
    "\n",
    "#tparams is dictionary to theano variables\n",
    "#pars is strings of parameters to include\n",
    "#R -> Parameters -> [String] -> R\n",
    "def weight_decay(decay_c, tparams, pars):\n",
    "    tdecay_c = theano.shared(numpy_floatX(decay_c), name='decay_c')\n",
    "    total = 0.\n",
    "    #do this in a for loop because the parameters may be different dimensions - so it's awkward to concatenate.\n",
    "    for name in pars:\n",
    "        total += (tparams[name] ** 2).sum()\n",
    "    return decay_c * total\n",
    "\n",
    "# Int -> Int -> Bool -> [(Int, [Int])], enumerated list of minibatch indices.\n",
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Used to shuffle the dataset at each iteration.\n",
    "    \"\"\"\n",
    "    #idx_list = [0..(n-1)]\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)\n",
    "\n",
    "#ERROR: add this\n",
    "def init_tparams(params):\n",
    "    tparams = OrderedDict()\n",
    "    for kk, pp in params.iteritems():\n",
    "        tparams[kk] = theano.shared(params[kk], name=kk)\n",
    "    return tparams\n",
    "\n",
    "# Dict String a -> Dict String (Theano a)\n",
    "def wrap_theano_dict(params, tparams=None):\n",
    "    \"\"\"\n",
    "    When we reload the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    if tparams==None: \n",
    "        #if no pointer to a dictionary given, create one\n",
    "        #TODO: initialize theano variables!\n",
    "        tparams = OrderedDict()\n",
    "    for kk, vv in params.iteritems():\n",
    "        tparams[kk].set_value(vv)\n",
    "    return tparams\n",
    "\n",
    "# Dict String (Theano a) -> Dict String a\n",
    "def unwrap_theano_dict(zipped):\n",
    "    \"\"\"\n",
    "    When we pickle the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    new_params = OrderedDict()\n",
    "    for kk, vv in zipped.iteritems():\n",
    "        new_params[kk] = vv.get_value()\n",
    "    return new_params\n",
    "\n",
    "def hot(choices, n):\n",
    "    return [n==x for x in range(choices)]\n",
    "\n",
    "#Int -> c:Int -> R^c\n",
    "def oneHot(choices, n):\n",
    "    #return [T.eq(n,x) for x in range(choices)]\n",
    "    return T.as_tensor_variable([T.eq(n,x) for x in range(choices)])\n",
    "\n",
    "#?\n",
    "def zipp(params, tparams):\n",
    "    \"\"\"\n",
    "    When we reload the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    for kk, vv in params.iteritems():\n",
    "        tparams[kk].set_value(vv)\n",
    "\n",
    "\n",
    "def mapped_oneHot(choices, ns):\n",
    "    return tmap(lambda x: oneHot(choices,x), ns)\n",
    "\n",
    "def mapped_mapped_oneHot(choices, nss):\n",
    "    return tmap2(lambda x: oneHot(choices,x), nss)\n",
    "\n",
    "def tmap(f, n, fixed=[]):\n",
    "    x, _ = theano.map(f, n, non_sequences=fixed)\n",
    "    return x\n",
    "\n",
    "def tmap2(f,n, fixed=[]):\n",
    "    return tmap(lambda x: tmap(f, x, non_sequences=fixed), n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utilites\n",
    "\n",
    "import itertools\n",
    "\n",
    "def case(var, li, else_expr=None):\n",
    "    for (val, expr) in li:\n",
    "        if val:\n",
    "            return expr\n",
    "    return else_expr\n",
    "\n",
    "#alternatively use the ternary control operator\n",
    "#    a if test else b\n",
    "#http://stackoverflow.com/questions/394809/does-python-have-a-ternary-conditional-operator\n",
    "#WARNING: THIS IS NOT LAZY\n",
    "def if_f(expr, t, f):\n",
    "    if expr:\n",
    "        return t\n",
    "    else:\n",
    "        return f\n",
    "\n",
    "def ifs(li, else_expr=None):\n",
    "    for (stmt, val) in li:\n",
    "        if stmt:\n",
    "            return val\n",
    "    return else_expr\n",
    "\n",
    "def concat(lis):\n",
    "    return itertools.chain(*lis)\n",
    "\n",
    "def union(*dicts):\n",
    "    return dict(sum(map(lambda dct: list(dct.items()), dicts), []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimizers\n",
    "\n",
    "from collections import OrderedDict\n",
    "import cPickle as pkl\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "from theano import config\n",
    "import theano.tensor as tensor\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "#import utilities\n",
    "#import nn_utilities\n",
    "\n",
    "\"\"\"\n",
    "Single steps for various optimizers\n",
    "Each optimizer returns two (compiled) theano functions (with updates)\n",
    "# f_grad_shared: calculates the cost, and updates its own parameters\n",
    "# f_updates: update the neural net weights\n",
    "Note cost is a Theano variable, not a compiled function.\n",
    "\"\"\"\n",
    "\n",
    "def sgd(lr, tparams, grads, cost, args):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    :note: A more complicated version of sgd then needed.  This is\n",
    "        done like that for adadelta and rmsprop.\n",
    "\n",
    "    \"\"\"\n",
    "    # New set of shared variable that will contain the gradient\n",
    "    # for a mini-batch.\n",
    "    gshared = [theano.shared(p.get_value() * 0., name='%s_grad' % k)\n",
    "               for k, p in tparams.iteritems()]\n",
    "    gsup = [(gs, g) for gs, g in zip(gshared, grads)]\n",
    "    #zip(gshared,grads)\n",
    "\n",
    "    # Function that computes gradients for a mini-batch, but do not\n",
    "    # updates the weights.\n",
    "    f_grad_shared = theano.function(args, cost, updates=gsup,\n",
    "                                    name='sgd_f_grad_shared')\n",
    "\n",
    "    pup = [(p, p - lr * g) for p, g in zip(tparams.values(), gshared)]\n",
    "\n",
    "    # Function that updates the weights from the previously computed\n",
    "    # gradient.\n",
    "    f_update = theano.function([lr], [], updates=pup,\n",
    "                               name='sgd_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update\n",
    "\n",
    "def adadelta(lr, tparams, grads, cost, args):\n",
    "    \"\"\"\n",
    "    An adaptive learning rate optimizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Theano SharedVariable\n",
    "        Initial learning rate\n",
    "    tpramas: Theano SharedVariable\n",
    "        Model parameters\n",
    "    grads: Theano variable\n",
    "        Gradients of cost w.r.t to parameres\n",
    "    x: Theano variable\n",
    "        Model inputs\n",
    "    mask: Theano variable\n",
    "        Sequence mask\n",
    "    y: Theano variable\n",
    "        Targets\n",
    "    cost: Theano variable\n",
    "        Objective fucntion to minimize\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For more information, see [ADADELTA]_.\n",
    "\n",
    "    .. [ADADELTA] Matthew D. Zeiler, *ADADELTA: An Adaptive Learning\n",
    "       Rate Method*, arXiv:1212.5701.\n",
    "    \"\"\"\n",
    "\n",
    "    zipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                  name='%s_grad' % k)\n",
    "                    for k, p in tparams.iteritems()]\n",
    "    running_up2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                 name='%s_rup2' % k)\n",
    "                   for k, p in tparams.iteritems()]\n",
    "    running_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                    name='%s_rgrad2' % k)\n",
    "                      for k, p in tparams.iteritems()]\n",
    "\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2))\n",
    "             for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    f_grad_shared = theano.function(args, cost, updates=zgup + rg2up,\n",
    "                                    name='adadelta_f_grad_shared')\n",
    "\n",
    "    updir = [-tensor.sqrt(ru2 + 1e-6) / tensor.sqrt(rg2 + 1e-6) * zg\n",
    "             for zg, ru2, rg2 in zip(zipped_grads,\n",
    "                                     running_up2,\n",
    "                                     running_grads2)]\n",
    "    ru2up = [(ru2, 0.95 * ru2 + 0.05 * (ud ** 2))\n",
    "             for ru2, ud in zip(running_up2, updir)]\n",
    "    param_up = [(p, p + ud) for p, ud in zip(tparams.values(), updir)]\n",
    "\n",
    "    f_update = theano.function([lr], [], updates=ru2up + param_up,\n",
    "                               on_unused_input='ignore',\n",
    "                               name='adadelta_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update\n",
    "\n",
    "\n",
    "def rmsprop(lr, tparams, grads, cost, args):\n",
    "    \"\"\"\n",
    "    A variant of  SGD that scales the step size by running average of the\n",
    "    recent step norms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Theano SharedVariable\n",
    "        Initial learning rate\n",
    "    tpramas: Theano SharedVariable\n",
    "        Model parameters\n",
    "    grads: Theano variable\n",
    "        Gradients of cost w.r.t to parameres\n",
    "    x: Theano variable\n",
    "        Model inputs\n",
    "    mask: Theano variable\n",
    "        Sequence mask\n",
    "    y: Theano variable\n",
    "        Targets\n",
    "    cost: Theano variable\n",
    "        Objective fucntion to minimize\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For more information, see [Hint2014]_.\n",
    "\n",
    "    .. [Hint2014] Geoff Hinton, *Neural Networks for Machine Learning*,\n",
    "       lecture 6a,\n",
    "       http://cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    zipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                  name='%s_grad' % k)\n",
    "                    for k, p in tparams.iteritems()]\n",
    "    running_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                   name='%s_rgrad' % k)\n",
    "                     for k, p in tparams.iteritems()]\n",
    "    running_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                    name='%s_rgrad2' % k)\n",
    "                      for k, p in tparams.iteritems()]\n",
    "\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    rgup = [(rg, 0.95 * rg + 0.05 * g) for rg, g in zip(running_grads, grads)]\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2))\n",
    "             for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    f_grad_shared = theano.function(args, cost,\n",
    "                                    updates=zgup + rgup + rg2up,\n",
    "                                    name='rmsprop_f_grad_shared')\n",
    "\n",
    "    updir = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                           name='%s_updir' % k)\n",
    "             for k, p in tparams.iteritems()]\n",
    "    updir_new = [(ud, 0.9 * ud - 1e-4 * zg / tensor.sqrt(rg2 - rg ** 2 + 1e-4))\n",
    "                 for ud, zg, rg, rg2 in zip(updir, zipped_grads, running_grads,\n",
    "                                            running_grads2)]\n",
    "    param_up = [(p, p + udn[1])\n",
    "                for p, udn in zip(tparams.values(), updir_new)]\n",
    "    f_update = theano.function([lr], [], updates=updir_new + param_up,\n",
    "                               on_unused_input='ignore',\n",
    "                               name='rmsprop_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update\n",
    "\n",
    "def train(\n",
    "    init_params, # initial parameters (not in Theano)\n",
    "    data_train, # : a (should be list of some sort)\n",
    "    data_valid, # : a\n",
    "    # data_test, # : a\n",
    "    batch_maker, # : Int -> a -> [[b]] \n",
    "        #function that given the batch size and data, returns a list of list of batch identifiers (ex. Int)\n",
    "    get_data_f, # : [[b]] -> (a -> train)\n",
    "        #function that given a list of list of batch identifiers, gives a function that takes the data and gives training\n",
    "    cost, # : (train -> Theano Float)\n",
    "        #cost function\n",
    "    pred_error,\n",
    "    args,\n",
    "    tparamss,\n",
    "    patience=10,  # Number of epoch to wait before early stop if no progress\n",
    "    max_epochs=5000,  # The maximum number of epoch to run,\n",
    "    dispFreq=10,  # Display to stdout the training progress every N updates\n",
    "    optimizer=rmsprop,\n",
    "    saveto='model.npz',\n",
    "    validFreq=370,  # Compute the validation error after this number of update.\n",
    "    saveFreq=1110,  # Save the parameters after every saveFreq updates\n",
    "    batch_size=16,  # The batch size during training.\n",
    "    valid_batch_size=64,  # The batch size used for validation/test set.\n",
    "):\n",
    "    print 'Building model'\n",
    "\n",
    "    ## initialize a theano variable dictionary with parameter values from init_params\n",
    "    ## tparamss = [wrap_theano_dict(init_param) for init_param in init_params]\n",
    "    ## careful of overlapping...\n",
    "\n",
    "    # tparamss is a list of dictionaries (CHECK THIS)\n",
    "    # ? Are these theano dicts?\n",
    "    tparams = union(*tparamss)\n",
    "\n",
    "    # ! use_noise is for dropout\n",
    "    \"\"\"(use_noise, x, mask,\n",
    "     y, f_pred_prob, f_pred, cost) = build_model(tparams, model_options)\"\"\"\n",
    "    \n",
    "    # Get values of tparams (dictionary of all parameters)\n",
    "    all_params=tparams.values()\n",
    "    ## concat([tparams.values() for tparams in tparamss])\n",
    "\n",
    "    # Compile the theano functions. \n",
    "    ## I don't think this commend is true - Note \"args\" contains information about the length of the vectors, so this effectively locks in the sequence length. IS THIS TRUE?\n",
    "    # Compile the cost function.\n",
    "    # cost : train -> Theano Float\n",
    "    f_cost = theano.function(args, cost, name='f_cost')\n",
    "\n",
    "    # Take the gradient, and compile that too.\n",
    "    print(\"grad\")\n",
    "    print(cost)\n",
    "    print(all_params)\n",
    "    grads = theano.gradient.grad(cost, wrt=all_params) #ERROR\n",
    "    f_grad = theano.function(args, grads, name='f_grad')\n",
    "\n",
    "    #learning rate\n",
    "    lr = tensor.scalar(name='lr')\n",
    "    # The optimizer takes as arguments the learning rate, parameter dictionary (of theano vars), gradient and cost function, and arguments to those functions. All inputs are theano variables.\n",
    "    # optimizer returns the f_grad_shared and the update function\n",
    "    # ? what does f_grad_shared do?\n",
    "    f_grad_shared, f_update = optimizer(lr, tparams, grads, cost, args)\n",
    "\n",
    "    # Now starting optimization\n",
    "    print 'Optimization'\n",
    "\n",
    "    # The data can be in two forms ([a], [b]) or [a].\n",
    "    # The length of ([a],[b]) or [a]\n",
    "    def _len(li_or_pair):\n",
    "        if type(li_or_pair)==\"tuple\":\n",
    "            return len(li_or_pair[0])\n",
    "        else:\n",
    "            return len(li_or_pair)\n",
    "\n",
    "    # If input is ([a],[b]), gives [a]; if input is [a] just gives [a]\n",
    "    def get_first_if_tuple(maybe_tuple):\n",
    "        if type(maybe_tuple)==\"tuple\":\n",
    "            return maybe_tuple[0]\n",
    "        else:\n",
    "            return maybe_tuple\n",
    "\n",
    "    # length of training data\n",
    "    l_train = _len(data_train)\n",
    "    l_valid = _len(data_valid)\n",
    "    ## Ignore test right now.\n",
    "    ## length of validation data\n",
    "    ## l_test = _len(data_test)\n",
    "\n",
    "    # batch_maker : a -> [[b]] \n",
    "    # function that given the data, returns a list of list of batch identifiers (ex. Int)\n",
    "    # this is for the validation data. We run batch_maker on the training data inside the epoch loop.\n",
    "    valid_batch_ids = batch_maker(valid_batch_size, data_valid)\n",
    "    #test_batch_ids = batch_maker(data_test)\n",
    "\n",
    "    # print the length of training data\n",
    "    print \"%d train examples\" % l_train\n",
    "    print \"%d valid examples\" % l_valid\n",
    "    #print \"%d test examples\" % l_test\n",
    "    \n",
    "    # initialize history_errs, which will contain the validation errors from each time it checks the validation error.\n",
    "    # best_p ?\n",
    "    # bad count to 0.\n",
    "    history_errs = []\n",
    "    best_p = None\n",
    "    bad_count = 0\n",
    "\n",
    "    # if no validation frequency is give, validate once an epoch\n",
    "    # (the length of an epoch is l_train / batch_size because each iteration ? takes batch_size samples.)\n",
    "    if validFreq == -1:\n",
    "        validFreq = l_train / batch_size\n",
    "    #if no save frequency is give, validate once an epoch\n",
    "    if saveFreq == -1:\n",
    "        saveFreq = l_train / batch_size\n",
    "\n",
    "    uidx = 0  # the number of updates done (increment by 1 every time we look at a batch and make an update)\n",
    "    estop = False  # early stop\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        #EPOCH LOOP\n",
    "        #epoch index. (An epoch means going through the data once.)\n",
    "        for eidx in range(max_epochs):\n",
    "            # ?\n",
    "            n_samples = 0\n",
    "            \n",
    "            # initialize epoch:\n",
    "            # Call batch_maker to partition the training data into batches.\n",
    "            # (ex. get the list of shuffled indices for the training set)\n",
    "            ## kf = get_minibatches_idx(l_train, batch_size, shuffle=True)\n",
    "            batch_ids = batch_maker(batch_size, data_train)\n",
    "            print(\"batch_ids\", batch_ids)\n",
    "            #BATCH LOOP\n",
    "            for batch_id in batch_ids: #ERROR\n",
    "                # batch_id is a batch (note batch_maker has the indices, not the actual data). Call get_data_f to get the actual data.\n",
    "                # increase number of updates done by 1\n",
    "                uidx += 1\n",
    "                ## use_noise.set_value(1.)\n",
    "                \n",
    "                # Select the random examples for this minibatch\n",
    "                \"\"\"\n",
    "                if type(train)==\"tuple\":\n",
    "                    inputs = map(lambda li: [li[t] for t in train_index], list(train))\n",
    "                else:\n",
    "                    #only 1 argument. also wrap up in single-element list for consistency.\n",
    "                    inputs = [[train[t] for t in train_index]]\n",
    "                n_samples += args[0].shape[0]\n",
    "                \"\"\"\n",
    "                print(\"batch_id\",batch_id)\n",
    "                # get the batch \n",
    "                # [[b]] -> (a -> train)\n",
    "                batch = get_data_f(data_train, batch_id)\n",
    "                ## n_samples += batch[0].shape[0]\n",
    "                # Expect batch to be a list or a tuple representing multiple arguments. If it's a single argument, wrap it in a list so we can use *batch to unpack the arguments.\n",
    "                if not isinstance(batch, (list, tuple)):\n",
    "                    batch = [batch]\n",
    "                # Compute the cost \n",
    "                cost = f_grad_shared(*batch)\n",
    "                # f_update updates the ? given the learning rate.\n",
    "                f_update(lrate)\n",
    "\n",
    "                # if the cost is infinite or undefined, stop.\n",
    "                if np.isnan(cost) or np.isinf(cost):\n",
    "                    print 'bad cost detected: ', cost\n",
    "                    return 1., 1., 1.\n",
    "\n",
    "                # Display if it's time to do so.\n",
    "                if np.mod(uidx, dispFreq) == 0:\n",
    "                    print 'Epoch ', eidx, 'Update ', uidx, 'Cost ', cost\n",
    "\n",
    "                # Save if it's time to do so.\n",
    "                if saveto and np.mod(uidx, saveFreq) == 0:\n",
    "                    print 'Saving...',\n",
    "                    \n",
    "                    # save the best parameters---not the current ones.\n",
    "                    if best_p is not None:\n",
    "                        params = best_p\n",
    "                    else:\n",
    "                        params = unwrap_theano_dict(tparams)\n",
    "                    \n",
    "                    # Save the arrays into \"saveto\" (which should be a .npz file).\n",
    "                    # Save the history of errors.\n",
    "                    # ? params are saved with the labels given by the dictionary.\n",
    "                    np.savez(saveto, history_errs=history_errs, **params)\n",
    "                    \n",
    "                    ## Warning: python2 notation\n",
    "                    # Dump the model options into [saveto].pkl.\n",
    "                    pkl.dump(model_options, open('%s.pkl' % saveto, 'wb'), -1)\n",
    "                    print 'Done'\n",
    "\n",
    "                # If it's time to validate\n",
    "                if np.mod(uidx, validFreq) == 0:\n",
    "                    ## use_noise.set_value(0.)\n",
    "                    # For each batch identifier in batch,\n",
    "                    # get that batch from data_train\n",
    "                    # and calculate prediction error.\n",
    "                    # Sum all these errors\n",
    "                    train_err = sum([pred_error(get_data_f(data_train, batch_id)) for batch_id in batch])/data_train.size[0]\n",
    "                    # Do the same for the validation error.\n",
    "                    valid_err = sum([pred_error(get_data_f(data_valid, batch_id)) for batch_id in batch_valid])/data_valid.size[0]\n",
    "                    ## test_err = sum([pred_error(get_data_f(data_test, batch_id)) for batch_id in batch])/data_test.size[0]\n",
    "                    \n",
    "                    # record the validtion error in the history.\n",
    "                    history_errs.append(valid_err) #[valid_err, test_err])\n",
    "\n",
    "                    # if the validation error is smaller than any seen so far\n",
    "                    if (best_p is None or\n",
    "                        valid_err <= np.array(history_errs)[:, 0].min()):\n",
    "                        # then save the parameters to best_p\n",
    "                        best_p = unzip(tparams)\n",
    "                        # set bad_counter to 0.\n",
    "                        bad_counter = 0\n",
    "                    \n",
    "                    # Show the training and validation error.\n",
    "                    print ('Train ', train_err, 'Valid ', valid_err) #,\n",
    "                           #'Test ', test_err)\n",
    "                    \n",
    "                    # If the current validation error is greater than the minimum validation error up to <patience> trials ago (why aren't we looking at the past few?), add 1 to bad_counter \n",
    "                    if (len(history_errs) > patience and\n",
    "                        valid_err >= np.array(history_errs)[:-patience, 0].min()):\n",
    "                        bad_counter += 1\n",
    "                        #If this happens more times than patience allows, then signal that we stopped early and stop.\n",
    "                        if bad_counter > patience:\n",
    "                            print 'Early Stop!'\n",
    "                            estop = True\n",
    "                            break\n",
    "            \n",
    "            # Show number of samples seen. \n",
    "            # ! This is currently 0.\n",
    "            print 'Seen %d samples' % n_samples\n",
    "\n",
    "            # If early stop activated, then stop.\n",
    "            if estop:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print \"Training interupted\"\n",
    "\n",
    "    end_time = time.time()\n",
    "    if best_p is not None:\n",
    "        # not sure what this does\n",
    "        # Puts the best_p (best parameters) into a dictionary?\n",
    "        zipp(best_p, tparams)\n",
    "    else:\n",
    "        best_p = unzip(tparams)\n",
    "\n",
    "##    use_noise.set_value(0.)\n",
    "    \n",
    "\n",
    "    ## kf_train_sorted = get_minibatches_idx(_len(train), batch_size)\n",
    "    # At the very end, calculate the training and validation error again.\n",
    "    kf_train_sorted = batch_maker(batch_size, data_train)\n",
    "    # Calculate training and validation error (note we also did this with [validFreq] frequency)\n",
    "    train_err = sum([pred_error(get_data_f(data_train, batch_id)) for batch_id in batch])/data_train.size[0]\n",
    "    valid_err = sum([pred_error(get_data_f(data_valid, batch_id)) for batch_id in batch_valid])/data_valid.size[0]\n",
    "\n",
    "    print 'Train ', train_err, 'Valid ', valid_err #, 'Test ', test_err\n",
    "\n",
    "    # Final save. Save the training error, validation error, history of errors, and the best parameters (unpacked)\n",
    "    if saveto:\n",
    "        np.savez(saveto, train_err=train_err,\n",
    "                    valid_err=valid_err, #test_err=test_err,\n",
    "                    history_errs=history_errs, **best_p)\n",
    "    # How long the code took to run\n",
    "    print 'The code run for %d epochs, with %f sec/epochs' % (\n",
    "        (eidx + 1), (end_time - start_time) / (1. * (eidx + 1)))\n",
    "    print >> sys.stderr, ('Training took %.1fs' %\n",
    "                          (end_time - start_time))\n",
    "    return train_err, valid_err #, test_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import *\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from collections import OrderedDict\n",
    "\n",
    "\"\"\"Parameters\"\"\"\n",
    "def unpack_params(tparams, li):\n",
    "    print(\"unpack\", tparams, li, type(tparams),type(li))\n",
    "    return [tparams[name] for name in li]\n",
    "\n",
    "\"\"\"Basic NN's\"\"\"\n",
    "def nn_layer1(x, W, b):\n",
    "    return T.dot(x,W) + b #ERROR x*W+b\n",
    "\n",
    "#R^m -> Parameters -> R^p, where W::R^{m x p} and b::R^p.\n",
    "def nn_layer(x, tparams):\n",
    "    W, b = unpack_params(tparams, [\"W\", \"b\"])\n",
    "    return nn_layer1(x, W, b)\n",
    "\n",
    "#R^k -> R^k -> R\n",
    "def logloss(pred, actual):\n",
    "    #sum on innermost axis.\n",
    "    return -(actual * corrected_log(pred)).sum(axis=-1)\n",
    "#CHECK THAT THIS MAPS\n",
    "\n",
    "#Warning: this doesn't map.\n",
    "#R^k -> Nat -> R\n",
    "def logloss_i(pred, actual_i):\n",
    "    return -corrected_log(pred[actual_i])\n",
    "\n",
    "#R -> R\n",
    "def corrected_log(x):\n",
    "    return T.log(T.maximum(1e-6,x)) #ERROR: maximum, not max\n",
    "\n",
    "#return dictionary of parameters (with default random initialization)\n",
    "def init_params_nn(n, m, init='zeros'):\n",
    "    rand_f = lambda l1, l2: np.asarray(np.random.normal(0, 1/np.sqrt(n), (l1,l2)))\n",
    "    (f, g) = case(init,\n",
    "                [('zeros', (np.zeros, lambda c, r: np.zeros((c, r)))),\n",
    "                 ('rand', (np.zeros, rand_f))])\n",
    "    return init_params_with_f_nn(n,m,f,g)\n",
    "\n",
    "#returns a dictionary of parameters, initialized using the functions f and g.\n",
    "def init_params_with_f_nn(n,m,f,g):\n",
    "    pairs = [(\"W\",np.asarray(g(n, m))),\n",
    "             (\"b\",np.asarray(f(m)))]\n",
    "    \"\"\"\n",
    "    pairs = [(\"W\",T.as_tensor_variable(np.asarray(g(n, m)))),\n",
    "             (\"b\",T.as_tensor_variable(np.asarray(f(m))))]\"\"\"\n",
    "    return OrderedDict(pairs)\n",
    "\n",
    "\n",
    "\"\"\"LSTM functions\"\"\"\n",
    "#x, C, h are the inputs, and C1, h1 as the outputs.\n",
    "#the rest are weight vectors.\n",
    "def step_lstm1(x, C, h, Wf, bf, Wi, bi, WC, bC, Wo, bo):\n",
    "    print(h)\n",
    "    print(x)\n",
    "    hx = T.concatenate([h,x],axis=-1) #dimension m+n\n",
    "    f = sigmoid(T.dot(hx, Wf) + bf) #dimension m\n",
    "    i = sigmoid(T.dot(hx, Wi) + bi) #dimension m\n",
    "    C_add = T.tanh(T.dot(hx, WC) + bC) #dimension m\n",
    "    C1 = f * C + i * C_add #dimension m\n",
    "    o = sigmoid(T.dot(hx, Wo) + bo) #dimension m\n",
    "    h1 = o * T.tanh(C1) #dimension m\n",
    "    return [C1, h1] #dimension 2m (as 2 separate lists)\n",
    "\n",
    "#the same function, but with the parameters grouped together.\n",
    "#R^n -> R^m -> R^m -> Parameters -> (R^m, R^m)\n",
    "def step_lstm(x, C, h, tparams): \n",
    "    Wf, bf, Wi, bi, WC, bC, Wo, bo = unpack_params(tparams, [\"Wf\", \"bf\", \"Wi\", \"bi\", \"WC\", \"bC\", \"Wo\", \"bo\"])\n",
    "    return step_lstm1(x, C, h, Wf, bf, Wi, bi, WC, bC, Wo, bo)\n",
    "\n",
    "#Now for scanning and mapping\n",
    "#1. unfold step_lstm into something that accepts and gives a sequence\n",
    "#2. make it something that will operate on a whole batch of sequences\n",
    "#R^m -> R^m -> R^{s x n} -> Parameters -> (R^{s x m}, R^{s x m})\n",
    "def sequence_lstm(C0, h0, xs, tparams):\n",
    "    #we need tparams because we need a link to the shared variables.\n",
    "    #CHECK: please check that this gives the weights in the right order.\n",
    "    print(\"in sequence_lstm\")\n",
    "    print('tparams', tparams)\n",
    "    Wf, bf, Wi, bi, WC, bC, Wo, bo = unpack_params(tparams, [\"Wf\", \"bf\", \"Wi\", \"bi\", \"WC\", \"bC\", \"Wo\", \"bo\"])\n",
    "    #the function fn should have arguments in the following order:\n",
    "    #sequences, outputs_info (accumulators), non_sequences\n",
    "    #(x, C, h, Wf, bf, Wi, bi, WC, bC, Wo, bo)\n",
    "    ([C_vals, h_vals], updates) = theano.scan(fn=step_lstm1,\n",
    "                                          sequences = xs, \n",
    "                                          outputs_info=[C0, h0], #initial values of the memory/accumulator\n",
    "                                          non_sequences=[Wf, bf, Wi, bi, WC, bC, Wo, bo]) #fixed parameters\n",
    "                                          #strict=True)\n",
    "    return [C_vals, h_vals]\n",
    "\n",
    "#play around with numpy to see how things map, to define step_multiple_lstm.\n",
    "\n",
    "def sequence_multiple_lstm1(Cs0, hs0, xss, tparams):\n",
    "    return tmap(f, [Cs0, hs0, xss], tparams)\n",
    "\n",
    "def step_multiple_lstm(xs, C, h, tparams):\n",
    "    #Everything maps automatically. (We've only used matrix multiplication and scalar functions like sigmoid.)\n",
    "    return step_lstm(xs, C, h, tparams)\n",
    "\n",
    "def sequence_multiple_lstm(Cs0, hs0, xss, tparams):\n",
    "    #Everything maps. Note xss, Cs0, hs0 must be Theano matrices, not a list of Theano lists.\n",
    "    #However, the input will be $((R^n)^k)^s$ so we need to switch axes.\n",
    "    #Dimensions count inwards        2  1  0\n",
    "    xss2 = xss.dimshuffle([1,0,2]) #ERROR. Check above logic.\n",
    "    return sequence_lstm(Cs0, hs0, xss2, tparams)\n",
    "\n",
    "\"\"\"Functions to evaluate the NN's and calculate loss\"\"\"\n",
    "#unmapped version. taking indices\n",
    "def fns_lstm(C0, h0, xis, yi, tparams1, tparams2):\n",
    "    #, last_only = True):\n",
    "    #evaluate the LSTM on this sequence\n",
    "    print('tparams1', tparams1)\n",
    "    [C_vals, h_vals] = sequence_lstm(C0, h0, xis, tparams1)\n",
    "    #it's simpler to get both the function for the last and the function for all\n",
    "    \"\"\" \n",
    "    if last_only:\n",
    "        h_vals = h_vals[-1]\n",
    "        C_vals = C_vals[-1]\n",
    "    \"\"\"\n",
    "    #feed into the neural net and get vector of activations\n",
    "    acts = nn_layer(h_vals, tparams2) #\n",
    "    #prediction is the argmax value. Take argmax along innermost (-1) axis\n",
    "    pred = T.argmax(acts, axis=-1)\n",
    "    #loss function\n",
    "    loss = logloss(acts, yi)\n",
    "    acts_last = acts[-1]\n",
    "    pred_last = pred[-1]\n",
    "    loss_last = loss[-1]\n",
    "    #1 if predicted next one correctly, 0 otherwise\n",
    "    acc_last = yi[pred_last]\n",
    "    return acts_last, pred_last, loss_last, acc_last, acts, pred, loss\n",
    "\n",
    "#is ALMOST THE SAME as above...\n",
    "def fns_multiple_lstm(b,m, xis, yi, (tparams1, tparams2)):\n",
    "    #C0 = T.matrix(\"C0\")\n",
    "    #h0 = T.matrix(\"h0\")\n",
    "    #ERROR: https://groups.google.com/forum/#!topic/theano-users/fSgdabbhmDg\n",
    "    #C0.tag.test_value = np.zeros((b,m))\n",
    "    #h0.tag.test_value = np.zeros((b,m))\n",
    "    C0 = T.as_tensor_variable(np.zeros((b,m))) #as tensor variable\n",
    "    h0 = T.as_tensor_variable(np.zeros((b,m)))\n",
    "    #evaluate the LSTM on this sequence\n",
    "    #? xis is b*s*n matrix, where n is dim of space of inputs, s is sequence length, and b is number in batch. \n",
    "    # Check ordering. \n",
    "    [C_vals, h_vals] = sequence_lstm(C0, h0, xis, tparams1)\n",
    "    # feed into the neural net and get vector of activations\n",
    "    # acts is b*s*n matrix (? hope it maps) Does this mean n is the same?\n",
    "    acts = nn_layer(h_vals, tparams2) #ERROR\n",
    "    # prediction is the argmax value. Take argmax along innermost (-1) axis\n",
    "    # pred is b*s vector\n",
    "    pred = T.argmax(acts, axis=-1)\n",
    "    #loss function\n",
    "    # loss is b*s matrix\n",
    "    loss = logloss(acts, yi)\n",
    "    # acts_last is b*s matrix \n",
    "    acts_last = acts[:,-1] #ERROR: replace ... with :\n",
    "    pred_last = pred[:,-1]\n",
    "    loss_last = loss[:,-1]\n",
    "    #1 if predicted next one correctly, 0 otherwise\n",
    "    #http://stackoverflow.com/questions/33929368/how-to-perform-a-range-on-a-theanos-tensorvariable\n",
    "    print(\"fns_multiple_lstm\")\n",
    "    print(xis.shape)\n",
    "    print(yi)\n",
    "    print(pred_last)\n",
    "    print(xis.shape[0])\n",
    "    # (0,pred_last[0]), (1, pred_last[1]),...\n",
    "    acc_last = yi[T.arange(xis.shape[0]),pred_last]\n",
    "    #http://stackoverflow.com/questions/23435782/numpy-selecting-specific-column-index-per-row-by-using-a-list-of-indexes\n",
    "    return acts_last, pred_last, loss_last, acc_last, acts, pred, loss\n",
    "\n",
    "#return dictionary of parameters (with default random initialization)\n",
    "def init_params_lstm(n, m, init='zeros'):\n",
    "    #normalize this!\n",
    "    rand_f = lambda l1, l2: np.asarray(np.random.normal(0, 1/np.sqrt(n), (l1,l2)))\n",
    "    (f, g) = case(init,\n",
    "                [('zeros', (np.zeros, lambda c, r: np.zeros((c, r)))),\n",
    "                 ('rand', (np.zeros, rand_f))])\n",
    "    return init_params_with_f_lstm(n,m,f,g)\n",
    "\n",
    "#returns a dictionary of parameters, initialized using the functions f and g.\n",
    "def init_params_with_f_lstm(n,m,f,g):\n",
    "    pairs = [(\"Wf\",np.asarray(g(m+n, m))),\n",
    "             (\"bf\",np.asarray(f(m))),\n",
    "             (\"Wi\",np.asarray(g(m+n, m))),\n",
    "             (\"bi\",np.asarray(f(m))),\n",
    "             (\"WC\",np.asarray(g(m+n, m))),\n",
    "             (\"bC\",np.asarray(f(m))),\n",
    "             (\"Wo\",np.asarray(g(m+n, m))), #ERROR\n",
    "             (\"bo\",np.asarray(f(m)))]\n",
    "    \"\"\"\n",
    "    pairs = [(\"Wf\",T.as_tensor_variable(np.asarray(g(m+n, m)))),\n",
    "             (\"bf\",T.as_tensor_variable(np.asarray(f(m)))),\n",
    "             (\"Wi\",T.as_tensor_variable(np.asarray(g(m+n, m)))),\n",
    "             (\"bi\",T.as_tensor_variable(np.asarray(f(m)))),\n",
    "             (\"WC\",T.as_tensor_variable(np.asarray(g(m+n, m)))),\n",
    "             (\"bC\",T.as_tensor_variable(np.asarray(f(m)))),\n",
    "             (\"Wo\",T.as_tensor_variable(np.asarray(g(m+n, n)))),\n",
    "             (\"bo\",T.as_tensor_variable(np.asarray(f(m))))]\n",
    "             \"\"\"\n",
    "    return OrderedDict(pairs)\n",
    "\n",
    "\"\"\"\n",
    "#Int^b -> R^{l * n} -> (R^{b * s * n}, R^{b * n})\n",
    "def get_data_f(indices, data):\n",
    "    #given indices, get the sequences in data starting at those indices.\n",
    "    #(seqs, ys)\\in R^{b*s*n} * R^{b*n}\n",
    "    return ([data[i:i+s] for i in indices], [data[i+s-1] for i in indices])\n",
    "    #does s include last? \n",
    "\"\"\"\n",
    "def get_data_f(li, batch_ids):\n",
    "    print(batch_ids)\n",
    "    _, bids = batch_ids\n",
    "    return ([li[i:i+s] for i in bids], [li[i+s] for i in bids])\n",
    "\n",
    "#li's are sequences, ex. [0,3,2,1,1,3,1]\n",
    "#the elements of the sequence are in [0..(n-1)], ex. n=4 above\n",
    "#m is the memory size\n",
    "#s is the sequence length, ex. 3 divides the above into [0,3,2],..,[1,3,1]. \n",
    "##li_test=[]\n",
    "def train_lstm(li_train, li_valid, n, m, s, batch_size, valid_batch_size=-1):\n",
    "    if valid_batch_size == -1:\n",
    "        valid_batch_size = batch_size\n",
    "    #turns li_train, etc. into one-hot vectors. (li_train is a list of characters.)\n",
    "    hot_li_train, hot_li_valid = [map(lambda x: hot(n, x), li) for li in [li_train, li_valid]]\n",
    "    ##hot_li_test\n",
    "    #n_seqs_train, n_seqs_valid, n_seqs_test = [len(li) - s + 1 for li in [li_train, li_valid, li_test]]\n",
    "    #note alternatively we can keep it as a single tensor...\n",
    "\n",
    "    def batch_maker(batch_size, data):\n",
    "        return get_minibatches_idx(len(data)-s, batch_size, shuffle=True)\n",
    "\n",
    "    # note this gives a tuple right now.\n",
    "    \"\"\"\n",
    "    def get_data_f(batch_ids, li):\n",
    "        print(batch_ids)\n",
    "        _, bids = batch_ids\n",
    "        return ([[li[x] for x in range(i, i+s)] for i in bids], [li[i+s] for i in bids])\n",
    "    \"\"\"\n",
    "\n",
    "    xis = T.dtensor3('xis')\n",
    "    yi = T.dmatrix('yi')\n",
    "    tparams1 = init_tparams(init_params_lstm(n,m,'rand'))\n",
    "    tparams2 = init_tparams(init_params_nn(m,n,'rand'))\n",
    "    #warning: locks in batch size number\n",
    "    _,_,loss,acc,_,_,_ = fns_multiple_lstm(batch_size, m, xis, yi, (tparams1, tparams2))\n",
    "    cost = loss.sum() #ERROR: add this\n",
    "    print(\"cost\",cost)\n",
    "    err = 1 - acc\n",
    "    #warning, these require m as argument.\n",
    "    #loss_f = function([xis,yi],loss)\n",
    "    #acc_f = function([xis,yi],acc)\n",
    "    #ERROR: '' around keys\n",
    "    arg_dict = {'init_params' : init_params_lstm(n, m),\n",
    "                'data_train' : hot_li_train, \n",
    "                'data_valid' : hot_li_valid,\n",
    "                'batch_maker' : batch_maker,\n",
    "                'get_data_f' : get_data_f,\n",
    "                'cost' : cost, \n",
    "                'pred_error' : err, \n",
    "                'args' : [xis,yi], \n",
    "                'tparamss' : [tparams1, tparams2], \n",
    "                'patience' : 10, \n",
    "                'max_epochs' : 5000, \n",
    "                'dispFreq' : 10, \n",
    "                'optimizer' : rmsprop,\n",
    "                'saveto' : 'lstm.npz',\n",
    "                'validFreq' : 500,\n",
    "                'saveFreq' : 1000,\n",
    "                'batch_size' : 16,\n",
    "                'valid_batch_size' : 64}\n",
    "                \n",
    "    train(**arg_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in sequence_lstm\n",
      "('tparams', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]))\n",
      "('unpack', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n",
      "<TensorType(float64, matrix)>\n",
      "xis[t]\n",
      "('unpack', OrderedDict([('W', W), ('b', b)]), ['W', 'b'], <class 'collections.OrderedDict'>, <type 'list'>)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /n/homeserver2/user2a/holdenl/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.7-Pisa-x86_64-2.7.10-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /n/homeserver2/user2a/holdenl/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.7-Pisa-x86_64-2.7.10-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fns_multiple_lstm\n",
      "Shape.0\n",
      "yi\n",
      "Subtensor{::, int64}.0\n",
      "Subtensor{int64}.0\n",
      "('cost', Sum{acc_dtype=float64}.0)\n",
      "Building model\n",
      "grad\n",
      "Sum{acc_dtype=float64}.0\n",
      "[Wf, bf, b, WC, bC, Wo, bo, bi, Wi, W]\n",
      "Optimization\n",
      "13 train examples\n",
      "8 valid examples\n",
      "('batch_ids', [(0, array([6, 8, 3, 2, 7, 1, 5, 9, 0, 4], dtype=int32))])\n",
      "('batch_id', (0, array([6, 8, 3, 2, 7, 1, 5, 9, 0, 4], dtype=int32)))\n",
      "(0, array([6, 8, 3, 2, 7, 1, 5, 9, 0, 4], dtype=int32))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly\nApply node that caused the error: Join(TensorConstant{1}, <TensorType(float64, matrix)>, xis[t])\nToposort index: 0\nInputs types: [TensorType(int8, scalar), TensorType(float64, matrix), TensorType(float64, matrix)]\n\nHINT: Use another linker then the c linker to have the inputs shapes and strides printed.\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{0}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, IncSubtensor{InplaceSet;:int64:}.0, Wf, Wi, WC, Wo, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0)\nToposort index: 129\nInputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, row), TensorType(float64, row), TensorType(float64, row), TensorType(float64, row)]\nInputs shapes: [(), (10, 3, 4), (11, 2, 3), (11, 2, 3), (7, 3), (7, 3), (7, 3), (7, 3), (1, 3), (1, 3), (1, 3), (1, 3)]\nInputs strides: [(), (96, 32, 8), (48, 24, 8), (48, 24, 8), (24, 8), (24, 8), (24, 8), (24, 8), (24, 8), (24, 8), (24, 8), (24, 8)]\nInputs values: [array(10), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array([[ 0.,  0.,  0.]]), array([[ 0.,  0.,  0.]]), array([[ 0.,  0.,  0.]]), array([[ 0.,  0.,  0.]])]\nOutputs clients: [[Subtensor{int64:int64:int64}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1}), Subtensor{int64:int64:int64}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})], [Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.1, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1}), Subtensor{int64:int64:int64}(forall_inplace,cpu,scan_fn}.1, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-404-6b00a17e9ef9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-401-da4dd4aaeab3>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[1;34m(li_train, li_valid, n, m, s, batch_size, valid_batch_size)\u001b[0m\n\u001b[0;32m    275\u001b[0m                 'valid_batch_size' : 64}\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-400-3994cc059969>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(init_params, data_train, data_valid, batch_maker, get_data_f, cost, pred_error, args, tparamss, patience, max_epochs, dispFreq, optimizer, saveto, validFreq, saveFreq, batch_size, valid_batch_size)\u001b[0m\n\u001b[0;32m    330\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                 \u001b[1;31m# Compute the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m                 \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_grad_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;31m# f_update updates the ? given the learning rate.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mf_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    938\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    939\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    927\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    930\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/n/homeserver2/user2a/holdenl/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.7-Pisa-x86_64-2.7.10-64/scan_perform/mod.cpp:4051)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/n/homeserver2/user2a/holdenl/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.7-Pisa-x86_64-2.7.10-64/scan_perform/mod.cpp:3975)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly\nApply node that caused the error: Join(TensorConstant{1}, <TensorType(float64, matrix)>, xis[t])\nToposort index: 0\nInputs types: [TensorType(int8, scalar), TensorType(float64, matrix), TensorType(float64, matrix)]\n\nHINT: Use another linker then the c linker to have the inputs shapes and strides printed.\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{0}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, IncSubtensor{InplaceSet;:int64:}.0, Wf, Wi, WC, Wo, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0)\nToposort index: 129\nInputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, row), TensorType(float64, row), TensorType(float64, row), TensorType(float64, row)]\nInputs shapes: [(), (10, 3, 4), (11, 2, 3), (11, 2, 3), (7, 3), (7, 3), (7, 3), (7, 3), (1, 3), (1, 3), (1, 3), (1, 3)]\nInputs strides: [(), (96, 32, 8), (48, 24, 8), (48, 24, 8), (24, 8), (24, 8), (24, 8), (24, 8), (24, 8), (24, 8), (24, 8), (24, 8)]\nInputs values: [array(10), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array([[ 0.,  0.,  0.]]), array([[ 0.,  0.,  0.]]), array([[ 0.,  0.,  0.]]), array([[ 0.,  0.,  0.]])]\nOutputs clients: [[Subtensor{int64:int64:int64}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1}), Subtensor{int64:int64:int64}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})], [Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.1, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1}), Subtensor{int64:int64:int64}(forall_inplace,cpu,scan_fn}.1, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "train_lstm([0,0,0,1,1,1,2,2,2,3,3,3,3], [0,0,1,1,2,2,3,3], 4, 3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D0 = T.matrix(\"D0\")\n",
    "D0.tag.test_value = np.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   4.   9.  16.  25.  36.  49.  64.  81.]\n",
      "[  0.00000000e+00   1.00000000e+00   1.60000000e+01   8.10000000e+01\n",
      "   2.56000000e+02   6.25000000e+02   1.29600000e+03   2.40100000e+03\n",
      "   4.09600000e+03   6.56100000e+03]\n"
     ]
    }
   ],
   "source": [
    "k = T.iscalar(\"k\")\n",
    "A = T.vector(\"A\")\n",
    "\n",
    "# Symbolic description of the result\n",
    "result, updates = theano.scan(fn=lambda prior_result, A: prior_result * A,\n",
    "                              outputs_info=T.ones_like(A),\n",
    "                              non_sequences=A,\n",
    "                              n_steps=k)\n",
    "\n",
    "final_result = result[-1]\n",
    "\n",
    "# compiled function that returns A**k\n",
    "power = theano.function(inputs=[A,k], outputs=final_result, updates=updates)\n",
    "\n",
    "print(power(range(10),2))\n",
    "print(power(range(10),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   4.   9.  16.  25.  36.  49.  64.  81.]\n",
      "[  0.00000000e+00   1.00000000e+00   1.60000000e+01   8.10000000e+01\n",
      "   2.56000000e+02   6.25000000e+02   1.29600000e+03   2.40100000e+03\n",
      "   4.09600000e+03   6.56100000e+03]\n"
     ]
    }
   ],
   "source": [
    "k = T.iscalar(\"k\")\n",
    "A = np.asarray(range(10))\n",
    "\n",
    "# Symbolic description of the result\n",
    "result, updates = theano.scan(fn=lambda prior_result, A: prior_result * A,\n",
    "                              outputs_info=np.ones(10),\n",
    "                              non_sequences=A,\n",
    "                              n_steps=k)\n",
    "\n",
    "final_result = result[-1]\n",
    "\n",
    "# compiled function that returns A**k\n",
    "power = theano.function(inputs=[k], outputs=final_result, updates=updates)\n",
    "\n",
    "print(power(2))\n",
    "print(power(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m=2\n",
    "n=3\n",
    "\n",
    "tparams1 = init_params_lstm(n,m,'rand')\n",
    "tparams2 = init_params_nn(m,n,'rand')\n",
    "\n",
    "C0 = np.zeros(m)\n",
    "h0 = np.zeros(m)\n",
    "\n",
    "xis = T.dmatrix('xis')\n",
    "yi = T.dvector('yi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tparams1', OrderedDict([('Wf', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bf', array([ 0.,  0.])), ('Wi', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bi', array([ 0.,  0.])), ('WC', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bC', array([ 0.,  0.])), ('Wo', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bo', array([ 0.,  0.]))]))\n",
      "in sequence_lstm\n",
      "('tparams', OrderedDict([('Wf', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bf', array([ 0.,  0.])), ('Wi', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bi', array([ 0.,  0.])), ('WC', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bC', array([ 0.,  0.])), ('Wo', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bo', array([ 0.,  0.]))]))\n",
      "('unpack', OrderedDict([('Wf', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bf', array([ 0.,  0.])), ('Wi', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bi', array([ 0.,  0.])), ('WC', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bC', array([ 0.,  0.])), ('Wo', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bo', array([ 0.,  0.]))]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n",
      "<TensorType(float64, vector)>\n",
      "xis[t]\n",
      "('unpack', OrderedDict([('W', array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]])), ('b', array([ 0.,  0.,  0.]))]), ['W', 'b'], <class 'collections.OrderedDict'>, <type 'list'>)\n"
     ]
    }
   ],
   "source": [
    "acts_last, pred_last, loss_last, acc_last, acts, pred, loss = fns_lstm(C0, h0, xis, yi, tparams1, tparams2)\n",
    "\n",
    "#https://groups.google.com/forum/#!topic/theano-users/74LA8It6ouI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clog(x):\n",
    "    return T.log(T.maximum(1e-6,x)) #ERROR: not max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = T.dvector('x')\n",
    "#x.tag.test_value = np.asarray([1,2])\n",
    "f = theano.function([x],tmap(clog,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = theano.function([x],clog(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69314718,  1.09861229])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69314718,  1.09861229])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-13.81551056,   0.        ,   0.69314718])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('W', array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])), ('b', array([ 0.,  0.,  0.]))])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tparams2 = init_params_nn(m,n,'rand')\n",
    "tparams2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unpack', OrderedDict([('W', array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]])), ('b', array([ 0.,  0.,  0.]))]), ['W', 'b'], <class 'collections.OrderedDict'>, <type 'list'>)\n"
     ]
    }
   ],
   "source": [
    "W, b = unpack_params(tparams2, [\"W\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 1, 'c': 1}"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = [{'c':1},{'a':2,'b':1}]\n",
    "d = union(*ds)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import cPickle as pkl\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "from theano import config\n",
    "import theano.tensor as tensor\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = 3\n",
    "\n",
    "\n",
    "def batch_maker(batch_size, data):\n",
    "        return get_minibatches_idx(len(data)-s, batch_size, shuffle=True)\n",
    "\n",
    "# note this gives a tuple right now.\n",
    "def get_data_f(batch_ids, li):\n",
    "    _, bids = batch_ids\n",
    "    return ([[li[x] for x in range(i, i+s)] for i in bids], [li[i+s] for i in bids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, array([ 3, 12], dtype=int32)), (1, array([ 8, 16], dtype=int32)), (2, array([ 7, 11], dtype=int32)), (3, array([ 4, 15], dtype=int32)), (4, array([ 1, 10], dtype=int32)), (5, array([14, 13], dtype=int32)), (6, array([2, 5], dtype=int32)), (7, array([6, 0], dtype=int32)), (8, array([9], dtype=int32))]\n",
      "(0, array([ 3, 12], dtype=int32))\n",
      "[ 3 12]\n",
      "([[3, 4, 5], [12, 13, 14]], [6, 15])\n"
     ]
    }
   ],
   "source": [
    "bids = batch_maker(2, range(20))\n",
    "print(bids)\n",
    "bid = bids[0]\n",
    "print(bid)\n",
    "_, bs = bid\n",
    "print(bs)\n",
    "print(get_data_f(bid, range(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([0, 1, 2], dtype=int32)), (1, array([3, 4, 5], dtype=int32))]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_minibatches_idx(6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Used to shuffle the dataset at each iteration.\n",
    "    \"\"\"\n",
    "    #idx_list = [0..(n-1)]\n",
    "    idx_list = numpy.arange(n, dtype=\"int32\")\n",
    "    \n",
    "    if shuffle:\n",
    "        numpy.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unpack', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n",
      "('tparams1', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]))\n",
      "in sequence_lstm\n",
      "('tparams', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]))\n",
      "('unpack', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n",
      "<TensorType(float64, vector)>\n",
      "xis[t]\n",
      "('unpack', OrderedDict([('W', W), ('b', b)]), ['W', 'b'], <class 'collections.OrderedDict'>, <type 'list'>)\n"
     ]
    }
   ],
   "source": [
    "m=2\n",
    "n=3\n",
    "tparams1 = init_tparams(init_params_lstm(n,m,'rand'))\n",
    "tparams2 = init_tparams(init_params_nn(m,n,'rand'))\n",
    "xis = T.dmatrix('xis')\n",
    "yi = T.dvector('yi')\n",
    "C0 = T.as_tensor_variable(np.zeros(m)) #as tensor variable\n",
    "h0 = T.as_tensor_variable(np.zeros(m))\n",
    "#print(type(tparams1['Wo']))\n",
    "#print(type(tparams2['W']))\n",
    "Wf, bf, Wi, bi, WC, bC, Wo, bo = unpack_params(tparams1, [\"Wf\", \"bf\", \"Wi\", \"bi\", \"WC\", \"bC\", \"Wo\", \"bo\"])\n",
    "#print(type(Wf))\n",
    "acts_last, pred_last, loss_last, acc_last, acts, pred, loss = fns_lstm(C0, h0, xis, yi, tparams1, tparams2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=theano.function([xis],acts_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([[0,0,1], [0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unpack', OrderedDict([('W', W), ('b', b)]), ['W', 'b'], <class 'collections.OrderedDict'>, <type 'list'>)\n"
     ]
    }
   ],
   "source": [
    "f= theano.function([x], nn_layer(x, tparams2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tparams2['W'].get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unpack', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n",
      "TensorConstant{(2,) of 0.0}\n",
      "x\n"
     ]
    }
   ],
   "source": [
    "f = theano.function([x], step_lstm(x, C0, h0, tparams1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.,  0.,  0.]), array([ 0.,  0.,  0.])]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unpack', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n"
     ]
    }
   ],
   "source": [
    "Wf, bf, Wi, bi, WC, bC, Wo, bo = unpack_params(tparams1, [\"Wf\", \"bf\", \"Wi\", \"bi\", \"WC\", \"bC\", \"Wo\", \"bo\"])\n",
    "h=T.as_tensor_variable(np.zeros(m))\n",
    "C=T.as_tensor_variable(np.zeros(m))\n",
    "hx = T.concatenate([h,x],axis=-1) #dimension m+n\n",
    "f = sigmoid(T.dot(hx, Wf) + bf) #dimension m\n",
    "i = sigmoid(T.dot(hx, Wi) + bi) #dimension m\n",
    "C_add = T.tanh(T.dot(hx, WC) + bC) #dimension m\n",
    "C1 = f * C + i * C_add #dimension m\n",
    "o = sigmoid(T.dot(hx, Wo) + bo) #dimension m\n",
    "h1 = o * T.tanh(C1) #dimension m\n",
    "\n",
    "f1 = theano.function([x], C1)\n",
    "f2 = theano.function([x], h1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  0.5,  0.5])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(hx.eval({x: [0,0,1]}))\n",
    "o.eval({x: [0,0,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1([0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2([0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in sequence_lstm\n",
      "('tparams', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]))\n",
      "('unpack', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n",
      "<TensorType(float64, vector)>\n",
      "xs[t]\n"
     ]
    }
   ],
   "source": [
    "xs = T.dmatrix('xs')\n",
    "Cval, hval = sequence_lstm(C0, h0, xs, tparams1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [ 0.,  0.]])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cval.eval({xs: [[0,0,1],[0,0,1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorType(float64, vector)>\n",
      "xs[t]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/holdenl/anaconda/lib/python2.7/site-packages/theano/scan_module/scan.py:1019: Warning: In the strict mode, all neccessary shared variables must be passed as a part of non_sequences\n",
      "  'must be passed as a part of non_sequences', Warning)\n"
     ]
    }
   ],
   "source": [
    "([C_vals, h_vals], updates) = theano.scan(fn=step_lstm1,\n",
    "                                          sequences = xs, \n",
    "                                          outputs_info=[C0, h0], #initial values of the memory/accumulator\n",
    "                                          non_sequences=[Wf, bf, Wi, bi, WC, bC, Wo, bo], #fixed parameters\n",
    "                                          strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unpack', OrderedDict([('Wf', Wf), ('bf', bf), ('Wi', Wi), ('bi', bi), ('WC', WC), ('bC', bC), ('Wo', Wo), ('bo', bo)]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wf, bf, Wi, bi, WC, bC, Wo, bo = unpack_params(tparams1, [\"Wf\", \"bf\", \"Wi\", \"bi\", \"WC\", \"bC\", \"Wo\", \"bo\"])\n",
    "h=T.as_tensor_variable(np.zeros(m))\n",
    "C=T.as_tensor_variable(np.zeros(m))\n",
    "hx = T.concatenate([h,x],axis=-1) #dimension m+n\n",
    "f = sigmoid(T.dot(hx, Wf) + bf) #dimension m\n",
    "i = sigmoid(T.dot(hx, Wi) + bi) #dimension m\n",
    "C_add = T.tanh(T.dot(hx, WC) + bC) #dimension m\n",
    "C1 = f * C + i * C_add #dimension m\n",
    "o = sigmoid(T.dot(hx, Wo) + bo) #dimension m\n",
    "h1 = o * T.tanh(C1) #dimension m\n",
    "\n",
    "f1 = theano.function([x], C1)\n",
    "f2 = theano.function([x], h1)\n",
    "f1([0,0,1])\n",
    "C1.eval({x:[0,0,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorConstant{(2,) of 0.0}\n",
      "x\n",
      "[ 0.  0.  0.  0.  1.]\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n",
      "[ 0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def step_lstm1(x, C, h, Wf, bf, Wi, bi, WC, bC, Wo, bo):\n",
    "print(h)\n",
    "print(x)\n",
    "hx = T.concatenate([h,x],axis=-1) #dimension m+n\n",
    "f = sigmoid(T.dot(hx, Wf) + bf) #dimension m\n",
    "i = sigmoid(T.dot(hx, Wi) + bi) #dimension m\n",
    "C_add = T.tanh(T.dot(hx, WC) + bC) #dimension m\n",
    "C1 = f * C + i * C_add #dimension m\n",
    "o = sigmoid(T.dot(hx, Wo) + bo) #dimension m\n",
    "h1 = o * T.tanh(C1) #dimension m\n",
    "    #return [C1, h1] #dimension 2m (as 2 separate lists)\n",
    "print(hx.eval({x:[0,0,1]}))  \n",
    "print(C_add.eval({x:[0,0,1]}))\n",
    "print(C1.eval({x:[0,0,1]}))\n",
    "print(h1.eval({x:[0,0,1]}))\n",
    "WC.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hx = T.concatenate([h0,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hx.eval({x:[0,0,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a named function, rather than using lambda\n",
    "def accumulate_by_adding(arange_val, sum_to_date):\n",
    "    return sum_to_date + arange_val\n",
    "seq = T.dmatrix(\"seq\")\n",
    "\n",
    "# An unauthorized implicit downcast from the dtype of 'seq', to that of\n",
    "# 'T.as_tensor_variable(0)' which is of dtype 'int8' by default would occur\n",
    "# if this instruction were to be used instead of the next one:\n",
    "# outputs_info = T.as_tensor_variable(0)\n",
    "\n",
    "#outputs_info = T.as_tensor_variable(np.asarray(0, seq.dtype))\n",
    "outputs_info = T.as_tensor_variable(np.zeros(2))\n",
    "scan_result, scan_updates = theano.scan(fn=accumulate_by_adding,\n",
    "                                        outputs_info=outputs_info,\n",
    "                                        sequences=seq)\n",
    "seqf = theano.function(inputs=[seq], outputs=scan_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.],\n",
       "       [ 5.,  7.]])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqf([[1,2],[4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2.,  5.,  9.]), array([  4.,  10.,  18.])]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accumulate_by_adding(arange_val, sum1, sum2):\n",
    "    return [sum1 + arange_val, sum2 + 2*arange_val]\n",
    "seq = T.dvector(\"seq\")\n",
    "\n",
    "# An unauthorized implicit downcast from the dtype of 'seq', to that of\n",
    "# 'T.as_tensor_variable(0)' which is of dtype 'int8' by default would occur\n",
    "# if this instruction were to be used instead of the next one:\n",
    "# outputs_info = T.as_tensor_variable(0)\n",
    "\n",
    "#outputs_info = T.as_tensor_variable(np.asarray(0, seq.dtype))\n",
    "outputs_info = T.as_tensor_variable(np.asarray(0, seq.dtype))\n",
    "scan_result, scan_updates = theano.scan(fn=accumulate_by_adding,\n",
    "                                        outputs_info=[outputs_info, outputs_info],\n",
    "                                        sequences=seq)\n",
    "seqf = theano.function(inputs=[seq], outputs=scan_result)\n",
    "seqf([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in sequence_lstm\n",
      "('tparams', OrderedDict([('Wf', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bf', array([ 0.,  0.])), ('Wi', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bi', array([ 0.,  0.])), ('WC', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bC', array([ 0.,  0.])), ('Wo', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bo', array([ 0.,  0.]))]))\n",
      "('unpack', OrderedDict([('Wf', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bf', array([ 0.,  0.])), ('Wi', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bi', array([ 0.,  0.])), ('WC', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bC', array([ 0.,  0.])), ('Wo', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bo', array([ 0.,  0.]))]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n",
      "<TensorType(float64, matrix)>\n",
      "xis[t]\n",
      "('unpack', OrderedDict([('W', array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]])), ('b', array([ 0.,  0.,  0.]))]), ['W', 'b'], <class 'collections.OrderedDict'>, <type 'list'>)\n",
      "fns_multiple_lstm\n",
      "Shape.0\n",
      "yi\n",
      "Subtensor{::, int64}.0\n",
      "Subtensor{int64}.0\n",
      "[ 13.81551056  13.81551056]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=2\n",
    "m=2\n",
    "xis = T.dtensor3('xis')\n",
    "yi = T.dmatrix('yi')\n",
    "acts_last, pred_last, loss_last, acc_last, acts, pred, loss = fns_multiple_lstm(b,m, xis, yi, (tparams1, tparams2))\n",
    "print(loss_last.eval({xis: [[[0,0,1],[0,0,1]], [[0,1,0],[0,0,1]]], yi: [[0,0,1],[0,1,0]]}))\n",
    "acts_last.eval({xis: [[[0,0,1],[0,0,1]]]})\n",
    "# , yi: [[0,0,1],[0,1,0]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acts_last.eval({xis: [[[0,0,1],[0,0,1]], [[0,1,0],[0,0,1]]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in sequence_lstm\n",
      "('tparams', OrderedDict([('Wf', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bf', array([ 0.,  0.])), ('Wi', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bi', array([ 0.,  0.])), ('WC', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bC', array([ 0.,  0.])), ('Wo', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bo', array([ 0.,  0.]))]))\n",
      "('unpack', OrderedDict([('Wf', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bf', array([ 0.,  0.])), ('Wi', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bi', array([ 0.,  0.])), ('WC', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bC', array([ 0.,  0.])), ('Wo', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bo', array([ 0.,  0.]))]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n",
      "<TensorType(float64, matrix)>\n",
      "<TensorType(float64, matrix)>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Theano Assert failed!\nApply node that caused the error: Assert{msg='Theano Assert failed!'}(TensorConstant{0.0}, Elemwise{eq,no_inplace}.0)\nToposort index: 8\nInputs types: [TensorType(float64, scalar), TensorType(int8, scalar)]\n\nHINT: Use another linker then the c linker to have the inputs shapes and strides printed.\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{1}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, IncSubtensor{InplaceSet;:int64:}.0)\nToposort index: 18\nInputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, 3D)]\nInputs shapes: [(), (2, 2, 3), (2, 2, 2), (1, 2, 2)]\nInputs strides: [(), (24, 48, 8), (32, 16, 8), (32, 16, 8)]\nInputs values: [array(2), 'not shown', 'not shown', array([[[ 0.,  0.],\n        [ 0.,  0.]]])]\nOutputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})], []]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-383-6bcacc12fd47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mC_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_vals\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence_multiple_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtparams1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/gof/graph.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, inputs_to_values)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minputs_to_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    938\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    939\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    927\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    930\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/n/homeserver2/user2a/holdenl/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.7-Pisa-x86_64-2.7.10-64/scan_perform/mod.cpp:4051)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/n/homeserver2/user2a/holdenl/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.7-Pisa-x86_64-2.7.10-64/scan_perform/mod.cpp:3975)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Theano Assert failed!\nApply node that caused the error: Assert{msg='Theano Assert failed!'}(TensorConstant{0.0}, Elemwise{eq,no_inplace}.0)\nToposort index: 8\nInputs types: [TensorType(float64, scalar), TensorType(int8, scalar)]\n\nHINT: Use another linker then the c linker to have the inputs shapes and strides printed.\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{1}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, IncSubtensor{InplaceSet;:int64:}.0)\nToposort index: 18\nInputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, 3D)]\nInputs shapes: [(), (2, 2, 3), (2, 2, 2), (1, 2, 2)]\nInputs strides: [(), (24, 48, 8), (32, 16, 8), (32, 16, 8)]\nInputs values: [array(2), 'not shown', 'not shown', array([[[ 0.,  0.],\n        [ 0.,  0.]]])]\nOutputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})], []]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "C0 = T.as_tensor_variable(np.zeros((b,m))) #as tensor variable\n",
    "h0 = T.as_tensor_variable(np.zeros((b,m)))\n",
    "#evaluate the LSTM on this sequence\n",
    "#? xis is b*s*n matrix, where n is dim of space of inputs, s is sequence length, and b is number in batch. \n",
    "# Check ordering. \n",
    "[C_vals, h_vals] = sequence_multiple_lstm(C0, h0, xis, tparams1)\n",
    "\n",
    "print(C_vals.eval({xis: [[[0,0,1],[0,0,1]],[[0,0,1],[0,0,1]]]}))\n",
    "print(h_vals.eval({xis: [[[0,0,1],[0,0,1]],[[0,0,1],[0,0,1]]]}))\n",
    "\n",
    "# feed into the neural net and get vector of activations\n",
    "# acts is b*s*n matrix (? hope it maps) Does this mean n is the same?\n",
    "acts = nn_layer(h_vals, tparams2) #ERROR\n",
    "\n",
    "print(acts.eval({xis: [[[0,0,1],[0,0,1]],[[0,0,1],[0,0,1]]]}))\n",
    "\n",
    "# prediction is the argmax value. Take argmax along innermost (-1) axis\n",
    "# pred is b*s vector\n",
    "pred = T.argmax(acts, axis=-1)\n",
    "\n",
    "print(pred.eval({xis: [[[0,0,1],[0,0,1]], [[0,0,1],[0,0,1]]]}))\n",
    "\n",
    "#loss function\n",
    "# loss is b*s matrix\n",
    "loss = logloss(acts, yi)\n",
    "\n",
    "print(loss.eval({xis: [[[0,0,1],[0,0,1]],[[0,0,1],[0,0,1]]], yi: [[0,0,1],[1,0,0]]}))\n",
    "\n",
    "# acts_last is b*s matrix \n",
    "acts_last = acts[:,-1] #ERROR: replace ... with :\n",
    "print(acts_last.eval({xis: [[[0,0,1],[0,0,1]],[[0,0,1],[0,0,1]]]}))\n",
    "\n",
    "pred_last = pred[:,-1]\n",
    "print(pred_last.eval({xis: [[[0,0,1],[0,0,1]],[[0,0,1],[0,0,1]]], yi: [[0,0,1],[1,0,0]]}))\n",
    "\n",
    "loss_last = loss[:,-1]\n",
    "print(loss_last.eval({xis: [[[0,0,1],[0,0,1]],[[0,0,1],[0,0,1]]], yi: [[0,0,1],[1,0,0]]}))\n",
    "\n",
    "#1 if predicted next one correctly, 0 otherwise\n",
    "#http://stackoverflow.com/questions/33929368/how-to-perform-a-range-on-a-theanos-tensorvariable\n",
    "print(\"fns_multiple_lstm\")\n",
    "print(xis.shape)\n",
    "print(yi)\n",
    "print(pred_last)\n",
    "print(xis.shape[0])\n",
    "# (0,pred_last[0]), (1, pred_last[1]),...\n",
    "acc_last = yi[T.arange(xis.shape[0]),pred_last]\n",
    "#http://stackoverflow.com/questions/23435782/numpy-selecting-specific-column-index-per-row-by-using-a-list-of-indexes\n",
    "return acts_last, pred_last, loss_last, acc_last, acts, pred, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xis2 = xis.dimshuffle([0,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DimShuffle{0,2,1}.0"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xis2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xis.dimshuffle([0,2,1])\n",
    "xis3 = xis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  1.],\n",
       "        [ 0.,  0.,  1.]],\n",
       "\n",
       "       [[ 0.,  1.,  0.],\n",
       "        [ 0.,  1.,  0.]]])"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xis3.eval({xis: [[[0,0,1],[0,0,1]],[[0,1,0],[0,1,0]]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 1.,  1.]],\n",
       "\n",
       "       [[ 0.,  0.],\n",
       "        [ 1.,  1.],\n",
       "        [ 0.,  0.]]])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xis2.eval({xis: [[[0,0,1],[0,0,1]],[[0,1,0],[0,1,0]]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.]]])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xis2 = xis.dimshuffle([1,0,2])\n",
    "xis2.eval({xis: [[[0,0,1],[0,0,1]],[[0,1,0],[0,1,0]]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unpack', OrderedDict([('Wf', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bf', array([ 0.,  0.])), ('Wi', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bi', array([ 0.,  0.])), ('WC', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bC', array([ 0.,  0.])), ('Wo', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])), ('bo', array([ 0.,  0.]))]), ['Wf', 'bf', 'Wi', 'bi', 'WC', 'bC', 'Wo', 'bo'], <class 'collections.OrderedDict'>, <type 'list'>)\n",
      "<TensorType(float64, matrix)>\n",
      "<TensorType(float64, matrix)>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Theano Assert failed!\nApply node that caused the error: Assert{msg='Theano Assert failed!'}(TensorConstant{0.0}, Elemwise{eq,no_inplace}.0)\nToposort index: 8\nInputs types: [TensorType(float64, scalar), TensorType(int8, scalar)]\n\nHINT: Use another linker then the c linker to have the inputs shapes and strides printed.\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{1}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, IncSubtensor{InplaceSet;:int64:}.0)\nToposort index: 18\nInputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, 3D)]\nInputs shapes: [(), (2, 2, 3), (2, 2, 2), (1, 2, 2)]\nInputs strides: [(), (24, 48, 8), (32, 16, 8), (32, 16, 8)]\nInputs values: [array(2), 'not shown', 'not shown', array([[[ 0.,  0.],\n        [ 0.,  0.]]])]\nOutputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})], []]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-388-3080f7bd0314>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                                       non_sequences=[Wf, bf, Wi, bi, WC, bC, Wo, bo]) #fixed parameters\n\u001b[0;32m     12\u001b[0m                                       \u001b[1;31m#strict=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mC_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/gof/graph.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, inputs_to_values)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minputs_to_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    938\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    939\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    927\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    930\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/n/homeserver2/user2a/holdenl/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.7-Pisa-x86_64-2.7.10-64/scan_perform/mod.cpp:4051)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/u/holdenl/anaconda/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/n/homeserver2/user2a/holdenl/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.7-Pisa-x86_64-2.7.10-64/scan_perform/mod.cpp:3975)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Theano Assert failed!\nApply node that caused the error: Assert{msg='Theano Assert failed!'}(TensorConstant{0.0}, Elemwise{eq,no_inplace}.0)\nToposort index: 8\nInputs types: [TensorType(float64, scalar), TensorType(int8, scalar)]\n\nHINT: Use another linker then the c linker to have the inputs shapes and strides printed.\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{1}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, IncSubtensor{InplaceSet;:int64:}.0)\nToposort index: 18\nInputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, 3D)]\nInputs shapes: [(), (2, 2, 3), (2, 2, 2), (1, 2, 2)]\nInputs strides: [(), (24, 48, 8), (32, 16, 8), (32, 16, 8)]\nInputs values: [array(2), 'not shown', 'not shown', array([[[ 0.,  0.],\n        [ 0.,  0.]]])]\nOutputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})], []]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "C0 = T.as_tensor_variable(np.zeros((b,m))) #as tensor variable\n",
    "h0 = T.as_tensor_variable(np.zeros((b,m)))\n",
    "\n",
    "Wf, bf, Wi, bi, WC, bC, Wo, bo = unpack_params(tparams1, [\"Wf\", \"bf\", \"Wi\", \"bi\", \"WC\", \"bC\", \"Wo\", \"bo\"])\n",
    "    #the function fn should have arguments in the following order:\n",
    "    #sequences, outputs_info (accumulators), non_sequences\n",
    "    #(x, C, h, Wf, bf, Wi, bi, WC, bC, Wo, bo)\n",
    "([C_vals, h_vals], updates) = theano.scan(fn=step_lstm1,\n",
    "                                      sequences = xis2, \n",
    "                                      outputs_info=[C0, h0], #initial values of the memory/accumulator\n",
    "                                      non_sequences=[Wf, bf, Wi, bi, WC, bC, Wo, bo]) #fixed parameters\n",
    "                                      #strict=True)\n",
    "C_vals.eval({xis: [[[0,0,1],[0,0,1]],[[0,1,0],[0,1,0]]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorConstant{(2, 2) of 0.0}\n",
      "x\n",
      "[[ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.]]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /n/homeserver2/user2a/holdenl/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.7-Pisa-x86_64-2.7.10-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /n/homeserver2/user2a/holdenl/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.7-Pisa-x86_64-2.7.10-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-397-893808f02a35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mWC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "#step for matrices\n",
    "h=h0\n",
    "C=C0\n",
    "x=T.dmatrix('x')\n",
    "print(h)\n",
    "print(x)\n",
    "hx = T.concatenate([h,x], axis=-1) #dimension m+n\n",
    "f = sigmoid(T.dot(hx, Wf) + bf) #dimension m\n",
    "i = sigmoid(T.dot(hx, Wi) + bi) #dimension m\n",
    "C_add = T.tanh(T.dot(hx, WC) + bC) #dimension m\n",
    "C1 = f * C + i * C_add #dimension m\n",
    "o = sigmoid(T.dot(hx, Wo) + bo) #dimension m\n",
    "h1 = o * T.tanh(C1) #dimension m\n",
    "    #return [C1, h1] #dimension 2m (as 2 separate lists)\n",
    "print(hx.eval({x: [[0,0,1],[0,1,0]]}))  \n",
    "print(C_add.eval({x: [[0,0,1],[0,1,0]]}))\n",
    "print(C1.eval({x: [[0,0,1],[0,1,0]]}))\n",
    "print(h1.eval(({x: [[0,0,1],[0,1,0]]})))\n",
    "WC.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  2.],\n",
       "       [ 2.,  3.,  4.]])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = T.dmatrix('a1')\n",
    "b1 = T.dmatrix('b1')\n",
    "c=T.concatenate([a1,b1], axis =1)\n",
    "c.eval({a1:[[1],[2]],b1: [[1,2],[3,4]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#it counts outside in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
